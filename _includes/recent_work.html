<h2 align="center">Selected recent work</h2>
<p align="center">See <a href="/publication">here</a> for a complete list.</p>

<hr class="hr-thick">

<div class="row">
   <div class="col-md-3">
      <img class="mr-4" src="/images/other/shallow_learning_sketch.png" width=100%>
   </div>
   <div class="col-md-9">
      <h3>Reverse engineering the neural tangent kernel</h3>
      <h5><strong>A first-principles method for the design of fully-connected architectures</strong></h5>
      <p class="blurb">
         Much of our understanding of artificial neural networks stems from the fact that, in the infinite-width limit, they turn out to be equivalent to a class of simple models called <i>kernel regression.</i> Given a wide network architecture, it's well-known how to find the equivalent kernel method, allowing us to study popular models in the infinite-width limit. We <i>invert this mapping</i> for fully-connected nets (FCNs), allowing one to start from a desired rotation-invariant kernel and design a network (i.e. choose an activation function) to achieve it. Remarkably, achieving any such kernel requires only one hidden layer, raising questions about conventional wisdom on the benefits of depth. This allows surprising experiments, like designing a 1HL FCN that trains and generalizes like a deep ReLU FCN. This ability to design nets with desired kernels is a step towards deriving good net architectures <i>from first principles</i>, a longtime dream of the field of machine learning.
      </p>
      <p class="paper-info">
         <strong>ICML '22</strong>
         <a href="https://arxiv.org/abs/2106.03186">[arXiv]</a>
         <a href="https://github.com/james-simon/reverse-engineering">[code]</a>
         <a href="https://james-simon.github.io/blog/reverse-engineering/">[blog]</a>
      </p>
   </div>
</div>